
- clarify the role of SMBS and them doing UAT - they should be accountable for it - platform should be fully functional when we hand it over to them
- the management of data is very important - we expect to have representative test data in the customer environments - see data migration
- check the concerns raised by Ryan about data
- we want to collaborate with vNext end empower them to do the testing of their configuration
- Anette is pushing for a holistic approach to testing that covers the entire client lifecycle
	- SMBS need to have clarity on what has been already tested to avoid useless repetitions

Generated by AI. Be sure to check for accuracy.
Meeting notes:
SC1 Testing Strategy Vision and Principles: 
Giuseppe presented the vision and principles for the SC1 Value Stream testing strategy, emphasizing that it is a work in progress and open for continuous improvement, with input from Ryan, Paul, Anette, and Aleksandra, and technical questions directed to Driss Biya.
	Strategy Overview: Giuseppe clarified that the presentation outlines the intended direction for SC1 testing, not the current state, and that the strategy is subject to ongoing refinement as challenges arise.
	Value Streams Ownership: Giuseppe explained that the Value Streams are responsible for the quality of configurations and client delivery, covering all testing activities from SC1DEV to client production environments.
	Continuous Improvement: The team acknowledged the need for adaptability and openness to change, with the strategy designed to evolve in response to both known and unknown challenges.
Scope of Testing and Standardization: 
Giuseppe described the scope of testing, focusing on the SC1 environment for standard configurations, with customer-specific environments used for non-standard parts, and discussed the importance of limiting customer-specific tests to critical scenarios, with input from Anette and Paul.
	Standard vs. Customer-Specific Testing: Testing is concentrated on the standard SC1 environment, while customer-specific environments are reserved for testing unique configurations, aiming to minimize redundant testing.
	Test Rationalization: Giuseppe emphasized rationalizing tests so that only critical, customer-specific scenarios are tested in client environments, relying on confidence built from standard tests.
	SMBS Role and UAT: Anette highlighted the requirement for SMBS and end clients to perform user acceptance testing, with Giuseppe confirming this is reflected in roles and responsibilities.
End-to-End Testing Approach: 
Giuseppe and Anette discussed the necessity of end-to-end testing focused on the entire client experience, ensuring workflows are tested as customers would use them, with Ryan raising questions about test data and integrations.
	Client Experience Focus: End-to-end testing is designed to replicate complete customer workflows, such as order processing, to ensure all components function as expected.
	Testing Challenges: Giuseppe noted that while modularization and mocking are common elsewhere, end-to-end tests are necessary for SC1, despite their complexity and cost.
	Test Data and Integration: Ryan and Anette raised concerns about managing test data and integrations in client environments, with Giuseppe acknowledging ongoing initiatives to address these challenges.
Testing of Deliverables and Quality Gates: 
Giuseppe and Paul discussed the process for testing deliverables, which are configuration packages moving from SC1DEV to SC1TEST to client environments, emphasizing the need for quality gates managed by delivery owners to prevent faulty configurations from reaching customers.
	Deliverables Definition: Deliverables are XMGR packages containing relevant configurations for functional areas, used to organize the flow from development to client environments.
	Quality Gate Implementation: Quality gates are intended to be controlled by delivery owners, ensuring that only validated configurations progress to clients, though this process is not yet fully in place.
	Business Validation: Paul stressed the importance of business validation and involvement of pre-sales and business experts in validating deliverables before they reach clients.
vNext Testing and Collaboration with PD: 
Giuseppe and Paul explained that vNext, the non-released version of Simcorp Dimension, is tested in PD and only moved to SC1 after version alignment, highlighting the need for collaboration with PD for holistic testing.
	vNext Testing Process: vNext configurations are tested in PD and integrated into SC1 only after release and version alignment, avoiding premature testing in SC1.
	Collaboration with PD: Paul and Giuseppe agreed on the importance of supporting PD in holistic testing of vNext to anticipate configuration issues before release.
Test Automation Strategy: 
Giuseppe, Paul, and Anette discussed the goal of automating all viable tests, the benefits of automation, and the current reliance on manual testing in customer environments, with input on balancing automation and manual testing.
	Automation Goals: The team aims to automate all tests where feasible, designing processes and systems to support automated testing in SC1.
	Manual Testing Role: Manual tests are necessary when automation is not viable, and currently all tests in customer environments are manual due to data challenges.
	Balancing Automation: Paul cautioned against automating all tests indiscriminately, suggesting a focus on key tests to avoid excessive test execution times.
	Continuous Testing: Anette noted that automated testing is already performed in client environments during upgrades, but alignment across teams and strategies is needed.
Alignment Across Testing Teams and Lifecycle: 
Anette, Ryan, and Paul discussed the need for a unified testing strategy across value streams, continuous testing, implementation, and SMBS teams to avoid redundant effort and ensure clarity on what has been tested, with agreement from Giuseppe.
	Unified Test Strategy: Anette emphasized the importance of having a single strategy covering onboarding, upgrade, and client lifecycle testing to prevent duplication and misalignment.
	Team Coordination: Ryan and Paul discussed explicit handovers and inclusion of all relevant teams, including implementation and SMBS, in the testing process.
	Test Reusability: The group agreed on the need for transparency and clarity on previously tested items to avoid unnecessary retesting and to ensure confidence for SMBS sign-off.
SC1DEV vs. SC1TEST Environments: 
Giuseppe explained the distinction between SC1DEV and SC1TEST environments, detailing where tests are developed, verified, and executed, and the importance of not modifying deliverable configuration objects during test execution.
	Environment Roles: SC1DEV is used for developing and verifying tests, while SC1TEST is for actual test execution, serving as a gate for deliverables.
	Configuration Integrity: Configuration objects in deliverables must remain unchanged during test execution to ensure consistency between tested artifacts and those delivered to clients.
Roles and Responsibilities in Testing: 
Giuseppe outlined the roles and responsibilities for outcomes configurators, Swiss Knives, and other stakeholders across SC1DEV, SC1TEST, client dev, client UAT, and client production environments, with input from Ryan and Aleksandra on onboarding and test data.
	Outcomes Configurators: Outcomes configurators define test requirements with Swiss Knives, execute manual tests in SC1DEV, support deliverable owners in SC1TEST, and verify functionality in client environments.
	Swiss Knives: Swiss Knives collaborate on test requirements, automate tests in SC1DEV, enable execution in SC1TEST, and provide general support in client environments.
	Onboarding and Test Data: Ryan and Aleksandra discussed onboarding new team members and the need for clear guidance on contributing to the C#-based test framework, as well as smarter approaches to test data management.
Test Data Management: 
Ryan and Giuseppe discussed the importance of strategic test data management, proposing that teams define data needs as part of configuration and test design, and aiming for reusable, environment-based test data rather than ad hoc creation.
	Strategic Data Approach: Ryan suggested that teams should design test data requirements alongside configuration and test cases, moving away from ad hoc data creation.
	Reusable Data Sets: The goal is to have blanket sets of test data in environments, allowing teams to select relevant subsets for their tests, improving efficiency and consistency.
	Support and Collaboration: Ryan offered support for obtaining and managing test data, emphasizing collaboration with commercial teams in the SIM Corp one live environment.
Reporting, Test Management, and Transparency: 
Anette and Giuseppe discussed the need for improved reporting, test management, and transparency across testing phases, including escalation strategies to address communication breakdowns and ensure clarity on what has been tested.
	Transparency Requirements: Anette highlighted the importance of transparency in test coverage, handovers, and reporting to avoid escalation and challenges in client testing.
	Escalation Strategy: Giuseppe proposed developing a clear escalation strategy to address communication breakdowns and ensure timely involvement of relevant stakeholders.
INTECH Pilot and Test Migration: 
Giuseppe explained the INTECH project as a pilot for implementing the new testing strategy, including migration of tests from other platforms into the SC1 framework, with Ryan and Anette noting the need for involvement and technical requirements for test automation.
	INTECH Pilot: The INTECH project serves as a pilot for the new testing strategy, involving testers and configurators to demonstrate practical implementation.
	Test Migration Process: Tests developed in other platforms can be migrated into SC1, requiring translation into the SC1 framework while maintaining functional integrity.
	Technical Automation Requirements: Automated tests must be repeatable, independent, and manage their own data to ensure optimal flow and reliability.
Follow-up tasks:
Test Data Management Strategy: 
Define and implement a smarter approach for managing and providing test data for configuration and test case design, ensuring teams do not have to manually create or source data for each test. (Ryan, Giuseppe)
Testing Strategy Alignment Across Teams: 
Develop and communicate a unified testing strategy that aligns onboarding, upgrade, and client environment testing, ensuring transparency on what has been tested, what can be rerun, and the delta scope for each phase. (Anette, Giuseppe)
Test Management and Reporting Transparency: 
Establish clear processes and tools for test management, reporting, and transparency, including tracking what is tested, where, and by whom, and ensuring proper escalation strategies are in place. (Giuseppe)
Onboarding and Training for Test Framework Contribution: 
Include training on how to contribute to the C#-based test framework in the upcoming onboarding sessions for new team members in early March. (Aleksandra, Ryan)